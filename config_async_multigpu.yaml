# async multi-gpu parallel training config
alg: fedasync
suffix: async_multigpu_test
resume_round: 7
device: [ 0,1 ]
test_device: [ 2,3 ]
dataset: cifar10
model: resnet18
model_in_cpu: false

# federated config
total_num: 20
sr: 0.2
rnd: 300  # 减少轮次用于测试
test_gap: 2

# local training config
bs: 64
epoch: 1
lr: 0.01
gamma: 0.99

# async specific config
decay: 0.3
a: 1
b: 4
strategy: hinge

# async multi-gpu parallel config
max_concurrent_per_device: 2  # 每个GPU最大并发训练数
aggregation_batch_size: 1     # 批量聚合的客户端数量
