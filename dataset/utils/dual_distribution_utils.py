"""
é€šç”¨åŒå±‚åˆ†å¸ƒæ§åˆ¶å·¥å…·
æ”¯æŒæ ‡ç­¾å±‚é¢å’Œç‰¹å¾å±‚é¢çš„ç‹¬ç«‹åˆ†å¸ƒæ§åˆ¶
é€‚ç”¨äºDomainNetç­‰å¤šç»´ç‰¹å¾æ•°æ®é›†
"""

import json
from typing import Any, Dict, List, Tuple

import numpy as np


def split_dual_distribution(
    data: np.ndarray, labels: np.ndarray, features: np.ndarray, config: Dict[str, Any]
) -> Tuple[List, List, Dict]:
    """
    é€šç”¨åŒå±‚åˆ†å¸ƒåˆ‡åˆ†å·¥å…·

    Args:
        data: åŸå§‹æ•°æ® [N, ...]
        labels: æ ‡ç­¾åˆ†å¸ƒ [N] (ç±»åˆ«ç´¢å¼•)
        features: ç‰¹å¾åˆ†å¸ƒ [N] (é£æ ¼ç­‰ç‰¹å¾ç´¢å¼•)
        config: é…ç½®å‚æ•°å­—å…¸ï¼ˆå¯åŒ…å« verbose å‚æ•°æ§åˆ¶æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼‰

    Returns:
        X: æ¯ä¸ªclientçš„æ•°æ®åˆ—è¡¨
        y: æ¯ä¸ªclientçš„æ ‡ç­¾åˆ—è¡¨
        statistic: è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«labelå’Œfeatureåˆ†å¸ƒ
    """
    num_clients = config["client_num"]
    batch_size = config["batch_size"]
    train_ratio = config["train_ratio"]
    verbose = config.get("verbose", False)  # ä»configä¸­è¯»å–verboseå‚æ•°

    # åˆ†å¸ƒæ§åˆ¶å‚æ•°
    label_partition = config.get("label_partition", "uni")  # 'uni' æˆ– 'dir'
    label_alpha = config.get("label_alpha", 10000)  # æ ‡ç­¾åˆ†å¸ƒçš„alphaå‚æ•°

    feature_partition = config.get("feature_partition", "uni")  # 'uni', 'dir', æˆ– 'pat'
    feature_p = config.get("feature_p", 2)  # patåˆ†å¸ƒä¸­æ¯ä¸ªclientçš„ç‰¹å¾ç§ç±»æ•°
    feature_pat_mode = config.get(
        "feature_pat_mode", "proportional"
    )  # patåˆ†å¸ƒçš„åˆ†é…æ¨¡å¼ï¼š'proportional' æˆ– 'uniform'

    # feature_alphaåªåœ¨ç‰¹å¾ç‹„åˆ©å…‹é›·åˆ†å¸ƒæ—¶éœ€è¦
    feature_alpha = None
    if feature_partition == "dir":
        feature_alpha = config.get("feature_alpha", 10000)  # ç‰¹å¾åˆ†å¸ƒçš„alphaå‚æ•°
    elif feature_partition == "pat":
        feature_alpha = config.get(
            "feature_alpha", 0.5
        )  # patåˆ†å¸ƒå†…éƒ¨æ ·æœ¬åˆ†é…çš„alphaå‚æ•°

    num_classes = len(np.unique(labels))
    num_features = len(np.unique(features))

    print(
        f"Dataset info: {len(data)} samples, {num_classes} classes, {num_features} features"
    )
    print(f"Label distribution: {label_partition} (alpha={label_alpha})")
    print(f"Feature distribution: {feature_partition}")
    if feature_alpha is not None:
        print(f"Feature alpha: {feature_alpha}")
    if feature_partition == "pat":
        print(f"Pathological distribution: each client has {feature_p} features")
        print(f"Pathological allocation mode: {feature_pat_mode}")

    X = [[] for _ in range(num_clients)]
    y = [[] for _ in range(num_clients)]
    f = [[] for _ in range(num_clients)]

    # è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ç»“æ„
    statistic = {
        "label_distribution": [[] for _ in range(num_clients)],
        "feature_distribution": [[] for _ in range(num_clients)],
        "client_stats": [{} for _ in range(num_clients)],
    }

    # ç¡®ä¿æœ€å°æ ·æœ¬æ•°
    least_samples = int(
        min(batch_size / (1 - train_ratio), len(labels) / num_clients / 2)
    )

    # åˆ›å»ºæ•°æ®ç´¢å¼•æ˜ å°„
    dataidx_map = {}

    if feature_partition == "pat":
        # Pathologicalåˆ†å¸ƒï¼šæ¯ä¸ªclientåªæœ‰pç§ç‰¹å¾
        # é¦–å…ˆç”Ÿæˆç‰¹å¾æ‹“æ‰‘ç»“æ„
        topology = _generate_pathological_topology(
            num_clients, num_features, features, feature_p, feature_pat_mode
        )
        # ç„¶ååœ¨ç‰¹å¾çº¦æŸä¸‹åˆ†é…æ ·æœ¬å’Œæ ‡ç­¾
        dataidx_map = _distribute_pathological_samples(
            topology,
            labels,
            features,
            num_clients,
            num_classes,
            label_partition,
            label_alpha,
        )

    elif feature_partition == "dir":
        # ç‰¹å¾å±‚é¢ç‹„åˆ©å…‹é›·åˆ†å¸ƒ
        dataidx_map = _split_feature_dirichlet(
            data, labels, features, num_clients, feature_alpha
        )

    elif feature_partition == "uni":  # feature_partition == 'uni'
        # ç‰¹å¾å±‚é¢å¹³å‡åˆ†å¸ƒ
        dataidx_map = _split_feature_uniform(data, labels, features, num_clients)
    else:
        raise ValueError(f"Unknown feature_partition: {feature_partition}")

    # åªæœ‰åœ¨épatæ¨¡å¼ä¸‹æ‰éœ€è¦é¢å¤–çš„æ ‡ç­¾åˆ†å¸ƒå¤„ç†
    # patæ¨¡å¼ä¸‹æ ‡ç­¾åˆ†å¸ƒå·²ç»åœ¨_distribute_pathological_samplesä¸­å¤„ç†è¿‡äº†
    if feature_partition != "pat":
        dataidx_map = _apply_label_distribution(
            dataidx_map,
            labels,
            num_clients,
            num_classes,
            label_partition,
            label_alpha,
            least_samples,
            feature_partition,
            features,
            dataidx_map,
        )

    # åˆ†é…æ•°æ®å¹¶æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
    for client in range(num_clients):
        if client in dataidx_map:
            idxs = dataidx_map[client]
            X[client] = data[idxs]
            y[client] = labels[idxs]
            f[client] = features[idxs]

            # æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡
            unique_labels, label_counts = np.unique(y[client], return_counts=True)
            for label, count in zip(unique_labels, label_counts):
                statistic["label_distribution"][client].append(
                    {"label": int(label), "count": int(count)}
                )

            # ç‰¹å¾åˆ†å¸ƒç»Ÿè®¡
            unique_features, feature_counts = np.unique(f[client], return_counts=True)
            for feature, count in zip(unique_features, feature_counts):
                statistic["feature_distribution"][client].append(
                    {"feature": int(feature), "count": int(count)}
                )

            # å®¢æˆ·ç«¯æ€»ä½“ç»Ÿè®¡
            statistic["client_stats"][client] = {
                "total_samples": len(X[client]),
                "num_labels": len(unique_labels),
                "num_features": len(unique_features),
                "unique_labels": unique_labels.tolist(),
                "unique_features": unique_features.tolist(),
            }

    # æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    _print_detailed_statistics(X, y, f, statistic, num_clients, verbose)

    # å¦‚æœæ˜¯patåˆ†å¸ƒï¼Œé¢å¤–æ‰“å°å…¨å±€åˆ†å¸ƒä¿¡æ¯
    if feature_partition == "pat":
        _print_pathological_global_distribution(f, num_features, num_clients, verbose)

    return X, y, statistic


def _generate_pathological_topology(
    num_clients: int,
    num_features: int,
    features: np.ndarray,
    feature_p: int,
    mode: str = "uniform",
) -> Dict[int, List[int]]:
    """
    ç”ŸæˆPathologicalåˆ†å¸ƒçš„ç‰¹å¾æ‹“æ‰‘ç»“æ„ï¼ˆå³æ¯ä¸ªclientæ‹¥æœ‰å“ªäº›ç‰¹å¾ï¼‰
    """
    # ç»Ÿè®¡æ¯ä¸ªç‰¹å¾çš„æ ·æœ¬æ•°é‡
    feature_sample_counts = {}
    for feature_id in range(num_features):
        feature_mask = features == feature_id
        feature_sample_counts[feature_id] = np.sum(feature_mask)

    # è®¡ç®—æ¯ä¸ªç‰¹å¾åº”è¯¥è¢«å¤šå°‘ä¸ªclientè¦†ç›–
    total_samples = sum(feature_sample_counts.values())
    feature_coverage = {}

    if mode == "proportional":
        # æ™ºèƒ½æ¯”ä¾‹åˆ†é…ï¼šå¤§æ ·æœ¬åŸŸåˆ†é…æ›´å¤šclient
        for feature_id, sample_count in feature_sample_counts.items():
            coverage = max(1, round((sample_count / total_samples) * num_clients))
            feature_coverage[feature_id] = coverage
    else:  # mode == 'uniform'
        # å‡åŒ€åˆ†é…ï¼šæ¯ä¸ªç‰¹å¾åˆ†é…ç›¸åŒæ•°é‡çš„client
        avg_coverage = max(1, num_clients // num_features)
        for feature_id in range(num_features):
            feature_coverage[feature_id] = avg_coverage

    # ä¸ºæ¯ä¸ªç‰¹å¾åˆ†é…client
    feature_client_assignment = {}
    for feature_id in range(num_features):
        # éšæœºé€‰æ‹©coverageä¸ªclient
        available_clients = list(range(num_clients))
        np.random.shuffle(available_clients)
        assigned_clients = available_clients[: feature_coverage[feature_id]]
        feature_client_assignment[feature_id] = assigned_clients

    # ä¸ºæ¯ä¸ªclientåˆ†é…ç‰¹å¾
    client_features = {i: [] for i in range(num_clients)}
    for feature_id, clients in feature_client_assignment.items():
        for client_id in clients:
            client_features[client_id].append(feature_id)

    # ä¸ºæ¯ä¸ªclientéšæœºé€‰æ‹©pä¸ªç‰¹å¾
    for client_id in range(num_clients):
        if len(client_features[client_id]) > feature_p:
            # å¦‚æœç‰¹å¾è¿‡å¤šï¼Œéšæœºé€‰æ‹©pä¸ª
            available_features = client_features[client_id]
            np.random.shuffle(available_features)
            client_features[client_id] = available_features[:feature_p]
        elif len(client_features[client_id]) < feature_p:
            # å¦‚æœç‰¹å¾ä¸è¶³ï¼Œä»æœªåˆ†é…çš„ç‰¹å¾ä¸­è¡¥å……
            current_features = set(client_features[client_id])
            unassigned_features = [
                f for f in range(num_features) if f not in current_features
            ]
            if unassigned_features:
                np.random.shuffle(unassigned_features)
                needed = feature_p - len(client_features[client_id])
                client_features[client_id].extend(unassigned_features[:needed])

    return client_features


def _distribute_pathological_samples(
    client_features: Dict[int, List[int]],
    labels: np.ndarray,
    features: np.ndarray,
    num_clients: int,
    num_classes: int,
    label_partition: str,
    label_alpha: float,
) -> Dict[int, List[int]]:
    """
    æ ¹æ®ç‰¹å¾æ‹“æ‰‘å’Œæ ‡ç­¾åˆ†å¸ƒç­–ç•¥åˆ†é…æ ·æœ¬
    åœ¨è¿™é‡Œå®ç°äº†çœŸæ­£çš„åŒå±‚åˆ†å¸ƒï¼š
    1. é¦–å…ˆç¡®ä¿æ¯ä¸ªclientåªæœ‰æŒ‡å®šçš„feature
    2. ç„¶ååœ¨featureçº¦æŸä¸‹æŒ‰label_partitionç­–ç•¥åˆ†é…æ ‡ç­¾
    """
    dataidx_map = {i: [] for i in range(num_clients)}

    # åå‘æ˜ å°„ï¼šç‰¹å¾ -> æ‹¥æœ‰è¯¥ç‰¹å¾çš„Clients
    feature_to_clients = {}
    for client_id, feats in client_features.items():
        for f_id in feats:
            if f_id not in feature_to_clients:
                feature_to_clients[f_id] = []
            feature_to_clients[f_id].append(client_id)

    num_features = len(np.unique(features))

    for feature_id in range(num_features):
        if feature_id not in feature_to_clients:
            continue

        clients_with_feature = feature_to_clients[feature_id]
        if not clients_with_feature:
            continue

        # è·å–è¯¥ç‰¹å¾çš„æ‰€æœ‰æ ·æœ¬ç´¢å¼•
        feature_mask = features == feature_id
        feature_indices = np.where(feature_mask)[0]

        # æŒ‰ç±»åˆ«åˆ†é…
        for class_id in range(num_classes):
            class_mask = labels[feature_indices] == class_id
            class_indices_in_feature = feature_indices[class_mask]

            if len(class_indices_in_feature) == 0:
                continue

            np.random.shuffle(class_indices_in_feature)

            # ç¡®å®šåˆ†é…æ¯”ä¾‹
            if label_partition == "dir":
                proportions = np.random.dirichlet(
                    [label_alpha] * len(clients_with_feature)
                )
            else:  # 'uni'
                proportions = np.array(
                    [1.0 / len(clients_with_feature)] * len(clients_with_feature)
                )

            # è®¡ç®—åˆ†å‰²ç‚¹
            split_points = (
                np.cumsum(proportions) * len(class_indices_in_feature)
            ).astype(int)[:-1]
            split_idxs = np.split(class_indices_in_feature, split_points)

            # åˆ†é…ç»™Client
            for i, client_id in enumerate(clients_with_feature):
                dataidx_map[client_id].extend(split_idxs[i].tolist())

    return dataidx_map


def _split_feature_dirichlet(
    data: np.ndarray,
    labels: np.ndarray,
    features: np.ndarray,
    num_clients: int,
    alpha: float,
) -> Dict[int, List[int]]:
    """ç‰¹å¾å±‚é¢ç‹„åˆ©å…‹é›·åˆ†å¸ƒ"""
    dataidx_map = {i: [] for i in range(num_clients)}
    num_features = len(np.unique(features))

    for feature_id in range(num_features):
        feature_mask = features == feature_id
        feature_indices = np.where(feature_mask)[0]
        np.random.shuffle(feature_indices)

        # ä½¿ç”¨ç‹„åˆ©å…‹é›·åˆ†å¸ƒåˆ†é…ç‰¹å¾åˆ°clients
        proportions = np.random.dirichlet([alpha] * num_clients)
        split_points = (np.cumsum(proportions) * len(feature_indices)).astype(int)[:-1]
        split_idxs = np.split(feature_indices, split_points)

        for client_id, idxs in enumerate(split_idxs):
            dataidx_map[client_id].extend(idxs.tolist())

    return dataidx_map


def _split_feature_uniform(
    data: np.ndarray, labels: np.ndarray, features: np.ndarray, num_clients: int
) -> Dict[int, List[int]]:
    """ç‰¹å¾å±‚é¢å¹³å‡åˆ†å¸ƒ"""
    dataidx_map = {i: [] for i in range(num_clients)}
    num_features = len(np.unique(features))

    for feature_id in range(num_features):
        feature_mask = features == feature_id
        feature_indices = np.where(feature_mask)[0]
        np.random.shuffle(feature_indices)

        # å¹³å‡åˆ†é…æ¯ä¸ªç‰¹å¾åˆ°æ‰€æœ‰clients
        split_size = len(feature_indices) // num_clients
        remainder = len(feature_indices) % num_clients

        start_idx = 0
        for client_id in range(num_clients):
            end_idx = start_idx + split_size + (1 if client_id < remainder else 0)
            dataidx_map[client_id].extend(feature_indices[start_idx:end_idx].tolist())
            start_idx = end_idx

    return dataidx_map


def _apply_label_distribution(
    dataidx_map: Dict[int, List[int]],
    labels: np.ndarray,
    num_clients: int,
    num_classes: int,
    label_partition: str,
    label_alpha: float,
    least_samples: int,
    feature_partition: str,
    features: np.ndarray,
    original_dataidx_map: Dict[int, List[int]],
) -> Dict[int, List[int]]:
    """åœ¨ç‰¹å¾åˆ†å¸ƒçš„åŸºç¡€ä¸Šåº”ç”¨æ ‡ç­¾åˆ†å¸ƒ"""

    if label_partition == "dir":
        # æ ‡ç­¾å±‚é¢ç‹„åˆ©å…‹é›·åˆ†å¸ƒ
        # å…¶ä»–æƒ…å†µä½¿ç”¨å…¨å±€dirichletåˆ†å¸ƒ
        return _apply_label_dirichlet(
            dataidx_map, labels, num_clients, num_classes, label_alpha, least_samples
        )
    else:  # label_partition == 'uni'
        # å¦‚æœå·²ç»è¿›è¡Œäº†ç‰¹å¾å±‚é¢çš„åˆ†é…ï¼ˆdataidx_mapä¸ä¸ºç©ºï¼‰ï¼Œä¸”è¦æ±‚æ ‡ç­¾uniï¼Œ
        # åˆ™åº”ä¿ç•™ç°æœ‰çš„ç‰¹å¾åˆ†é…ï¼Œè€Œä¸æ˜¯é‡æ–°è¿›è¡Œå…¨å±€uniåˆ†é…ã€‚
        if (
            dataidx_map
            and len(dataidx_map) > 0
            and any(len(v) > 0 for v in dataidx_map.values())
        ):
            return dataidx_map

        # æ ‡ç­¾å±‚é¢å¹³å‡åˆ†å¸ƒ
        return _apply_label_uniform(dataidx_map, labels, num_clients, num_classes)


def _apply_label_dirichlet(
    dataidx_map: Dict[int, List[int]],
    labels: np.ndarray,
    num_clients: int,
    num_classes: int,
    alpha: float,
    least_samples: int,
) -> Dict[int, List[int]]:
    """åº”ç”¨æ ‡ç­¾å±‚é¢ç‹„åˆ©å…‹é›·åˆ†å¸ƒ"""
    new_dataidx_map = {i: [] for i in range(num_clients)}

    for class_id in range(num_classes):
        # æ‰¾åˆ°è¯¥ç±»åˆ«çš„æ‰€æœ‰æ ·æœ¬
        class_mask = labels == class_id
        class_indices = np.where(class_mask)[0]

        # ä¸ºæ¯ä¸ªclassåˆ›å»ºclientåˆ†é…æ¯”ä¾‹
        proportions = np.random.dirichlet([alpha] * num_clients)
        split_points = (np.cumsum(proportions) * len(class_indices)).astype(int)[:-1]
        split_idxs = np.split(class_indices, split_points)

        # åˆ†é…åˆ°clients
        for client_id, idxs in enumerate(split_idxs):
            new_dataidx_map[client_id].extend(idxs.tolist())

    # ç§»é™¤æ ·æœ¬æ•°è¿‡å°‘çš„client
    min_size = least_samples
    filtered_map = {}
    for client_id, idxs in new_dataidx_map.items():
        if len(idxs) >= min_size:
            filtered_map[client_id] = idxs

    # å¦‚æœè¿‡æ»¤åclientæ•°é‡å¤ªå°‘ï¼Œé‡æ–°åˆ†é…
    if len(filtered_map) < num_clients // 2:
        # ç®€åŒ–åˆ†é…ç­–ç•¥
        filtered_map = {i: [] for i in range(num_clients)}
        for class_id in range(num_classes):
            class_mask = labels == class_id
            class_indices = np.where(class_mask)[0]
            np.random.shuffle(class_indices)

            split_size = len(class_indices) // num_clients
            remainder = len(class_indices) % num_clients

            start_idx = 0
            for client_id in range(num_clients):
                end_idx = start_idx + split_size + (1 if client_id < remainder else 0)
                filtered_map[client_id].extend(
                    class_indices[start_idx:end_idx].tolist()
                )
                start_idx = end_idx

    return filtered_map


def _apply_label_uniform(
    dataidx_map: Dict[int, List[int]],
    labels: np.ndarray,
    num_clients: int,
    num_classes: int,
) -> Dict[int, List[int]]:
    """åº”ç”¨æ ‡ç­¾å±‚é¢å¹³å‡åˆ†å¸ƒ"""
    new_dataidx_map = {i: [] for i in range(num_clients)}

    for class_id in range(num_classes):
        # æ‰¾åˆ°è¯¥ç±»åˆ«çš„æ‰€æœ‰æ ·æœ¬
        class_mask = labels == class_id
        class_indices = np.where(class_mask)[0]
        np.random.shuffle(class_indices)

        # å¹³å‡åˆ†é…åˆ°æ‰€æœ‰clients
        split_size = len(class_indices) // num_clients
        remainder = len(class_indices) % num_clients

        start_idx = 0
        for client_id in range(num_clients):
            end_idx = start_idx + split_size + (1 if client_id < remainder else 0)
            new_dataidx_map[client_id].extend(class_indices[start_idx:end_idx].tolist())
            start_idx = end_idx

    return new_dataidx_map


def _print_detailed_statistics(
    X: List, y: List, f: List, statistic: Dict, num_clients: int, verbose: bool = False
):
    """æ‰“å°è¯¦ç»†çš„clientç»Ÿè®¡ä¿¡æ¯"""

    if verbose:
        print("\n" + "=" * 80)
        print("DETAILED CLIENT DISTRIBUTION STATISTICS")
        print("=" * 80)

        for client in range(num_clients):
            if len(X[client]) > 0:
                print(f"Client {client}:")
                print(f"  ğŸ“Š Total samples: {len(X[client])}")
                print(
                    f"  ğŸ·ï¸  Unique labels: {statistic['client_stats'][client]['num_labels']} (IDs: {statistic['client_stats'][client]['unique_labels']})"
                )
                print(
                    f"  ğŸ¨ Unique features: {statistic['client_stats'][client]['num_features']} (IDs: {statistic['client_stats'][client]['unique_features']})"
                )

                # æ ‡ç­¾åˆ†å¸ƒè¯¦æƒ…
                print(f"  ğŸ“ˆ Label distribution:")
                for label_info in statistic["label_distribution"][client]:
                    print(
                        f"     Label {label_info['label']}: {label_info['count']} samples"
                    )

                # ç‰¹å¾åˆ†å¸ƒè¯¦æƒ…
                print(f"  ğŸ¯ Feature distribution:")
                for feature_info in statistic["feature_distribution"][client]:
                    print(
                        f"     Feature {feature_info['feature']}: {feature_info['count']} samples"
                    )

                print("-" * 60)
            else:
                print(f"Client {client}: âŒ No data assigned")
                print("-" * 60)

        print("=" * 80)

    # å…¨å±€ç»Ÿè®¡æ‘˜è¦
    total_samples = sum(stat["total_samples"] for stat in statistic["client_stats"])
    print(f"\nğŸ“Š GLOBAL SUMMARY:")
    print(f"   Total samples across all clients: {total_samples}")
    print(f"   Average samples per client: {total_samples / num_clients:.1f}")
    print(
        f"   Number of clients with data: {sum(1 for stat in statistic['client_stats'] if stat['total_samples'] > 0)}"
    )
    print("=" * 80)


def _print_pathological_global_distribution(
    f: List, num_features: int, num_clients: int, verbose: bool = False
):
    """æ‰“å°Pathologicalåˆ†å¸ƒçš„å…¨å±€ä¿¡æ¯"""
    if not verbose:
        return

    print("\n" + "=" * 80)
    print("PATHOLOGICAL DISTRIBUTION GLOBAL VIEW")
    print("=" * 80)

    # ç»Ÿè®¡æ¯ä¸ªç‰¹å¾è¢«å“ªäº›clientæ‹¥æœ‰
    feature_clients = {}
    for feature_id in range(num_features):
        feature_clients[feature_id] = []

    for client_id in range(num_clients):
        if len(f[client_id]) > 0:
            unique_features = np.unique(f[client_id])
            for feature_id in unique_features:
                if feature_id not in feature_clients:
                    feature_clients[feature_id] = []
                feature_clients[feature_id].append(client_id)

    # æ‰“å°æ¯ä¸ªç‰¹å¾çš„åˆ†å¸ƒ
    print("ğŸ” FEATURE VIEW (which clients have each feature):")
    for feature_id in range(num_features):
        clients_with_feature = feature_clients.get(feature_id, [])
        print(
            f"Feature {feature_id}: covered by {len(clients_with_feature)} clients -> {clients_with_feature}"
        )

    print("\nğŸ” CLIENT VIEW (which features does each client have):")
    # æ‰“å°æ¯ä¸ªclientæ‹¥æœ‰å“ªäº›ç‰¹å¾
    for client_id in range(num_clients):
        if len(f[client_id]) > 0:
            unique_features = np.unique(f[client_id])
            print(
                f"Client {client_id}: has {len(unique_features)} features -> {unique_features.tolist()}"
            )
        else:
            print(f"Client {client_id}: âŒ No data assigned")

    print("=" * 80)


def save_detailed_statistics(
    statistic: Dict, save_path: str, domain_names: List[str] = None
):
    """
    ä¿å­˜è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶

    Args:
        statistic: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        save_path: ä¿å­˜è·¯å¾„
        domain_names: åŸŸååˆ—è¡¨ï¼ˆç”¨äºç‰¹å¾IDåˆ°åç§°çš„æ˜ å°„ï¼‰
    """
    # å¦‚æœæä¾›äº†åŸŸååˆ—è¡¨ï¼Œè½¬æ¢ç‰¹å¾IDä¸ºåç§°
    if domain_names:
        for client_stats in statistic["feature_distribution"]:
            for feature_info in client_stats:
                if feature_info["feature"] < len(domain_names):
                    feature_info["feature_name"] = domain_names[feature_info["feature"]]

    # ä¿å­˜ä¸ºJSONæ ¼å¼
    json_path = save_path.replace(".yaml", ".json").replace(".yml", ".json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(statistic, f, indent=2, ensure_ascii=False)

    # ä¿å­˜ä¸ºYAMLæ ¼å¼
    yaml_path = save_path
    try:
        import yaml

        with open(yaml_path, "w", encoding="utf-8") as f:
            yaml.dump(
                statistic, f, default_flow_style=False, allow_unicode=True, indent=2
            )
    except ImportError:
        print("Warning: yaml module not available, skipping YAML export")

    print(f"ğŸ“ Detailed statistics saved to:")
    print(f"   JSON: {json_path}")
    if domain_names:
        print(f"   YAML: {yaml_path}")

    return json_path, yaml_path if domain_names else None
