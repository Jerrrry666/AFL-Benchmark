"""
DomainNetæ•°æ®é›†ç”Ÿæˆå™¨
æ”¯æŒåŒå±‚noniidåˆ†å¸ƒï¼šæ ‡ç­¾å±‚é¢å’Œé£æ ¼å±‚é¢çš„ç‹¬ç«‹åˆ†å¸ƒæ§åˆ¶
"""

import json
import random
from pathlib import Path

import numpy as np
import yaml
from PIL import Image
from tqdm import tqdm

from utils.dataset_utils import check, save_file, split_data
from utils.dual_distribution_utils import save_detailed_statistics, split_dual_distribution

random.seed(1)
np.random.seed(1)

# DomainNetç‰¹å®šé…ç½®
ROOT_DIR = '/home/mayiming/Dataset/DomainNet/'
DOMAIN_NAMES = ['clipart', 'infograph', 'painting', 'quickdraw', 'real', 'sketch']
NUM_CLASSES_PER_DOMAIN = 345


class DomainNetLoader:
    """DomainNetæ•°æ®é›†åŠ è½½å™¨"""

    def __init__(self, root_dir=ROOT_DIR):
        self.root_dir = Path(root_dir)
        self.domain_names = DOMAIN_NAMES
        self.num_domains = len(self.domain_names)
        self.num_classes_per_domain = NUM_CLASSES_PER_DOMAIN

        # ç®€åŒ–çš„ç¼“å­˜æ–‡ä»¶è·¯å¾„
        self.cache_data_path = self.root_dir / "cached_data.npz"
        self.cache_metadata_path = self.root_dir / "cache_metadata.json"

    def _is_cache_valid(self):
        """ç®€å•æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        return self.cache_data_path.exists() and self.cache_metadata_path.exists()

    def _save_cache(self, data, labels, domains):
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜"""
        print("ğŸ’¾ Saving data to cache...")

        # ä¿å­˜æ•°æ®
        np.savez_compressed(str(self.cache_data_path),
                            data=data,
                            labels=labels,
                            domains=domains)

        # ä¿å­˜ç®€åŒ–çš„å…ƒæ•°æ®
        cache_metadata = {
            'cache_time'   : str(np.datetime64('now')),
            'total_samples': len(data),
            'num_classes'  : len(np.unique(labels)),
            'num_domains'  : len(np.unique(domains))
        }

        with open(self.cache_metadata_path, 'w') as f:
            json.dump(cache_metadata, f, indent=2)

        print(f"âœ… Cache saved to {self.cache_data_path}")

    def _load_from_cache(self):
        """ä»ç¼“å­˜åŠ è½½æ•°æ®"""
        print("ğŸ“‚ Loading data from cache...")

        try:
            cache_data = np.load(str(self.cache_data_path), allow_pickle=True)
            data = cache_data['data']
            labels = cache_data['labels']
            domains = cache_data['domains']

            print(
                    f"âœ… Cache loaded: {len(data)} samples, {len(np.unique(labels))} classes, {len(np.unique(domains))} domains")
            return data, labels, domains

        except Exception as e:
            print(f"âŒ Failed to load cache: {e}")
            return None, None, None

    def load_dataset(self):
        """åŠ è½½DomainNetæ•°æ®é›†"""
        print("ğŸ” Checking for cached data...")

        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if self._is_cache_valid():
            data, labels, domains = self._load_from_cache()
            if data is not None:
                return data, labels, domains

        print("ğŸ“ No valid cache found, scanning filesystem...")
        print("Loading DomainNet dataset...")

        all_data = []
        all_labels = []
        all_domains = []

        for domain_idx, domain_name in enumerate(self.domain_names):
            domain_path = self.root_dir / domain_name

            if not domain_path.exists():
                print(f"âš ï¸ Warning: Domain path {domain_path} does not exist!")
                continue

            print(f"ğŸ“‚ Loading domain: {domain_name}")

            # è·å–è¯¥domainä¸‹çš„æ‰€æœ‰ç±»åˆ«æ–‡ä»¶å¤¹
            class_dirs = [d for d in domain_path.iterdir() if d.is_dir()]
            class_dirs = sorted(class_dirs)  # ç¡®ä¿é¡ºåºä¸€è‡´

            for class_idx, class_dir in enumerate(tqdm(class_dirs, desc=f"Classes in {domain_name}")):
                class_name = class_dir.name

                # è·å–è¯¥ç±»åˆ«ä¸‹çš„æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
                img_files = []
                for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
                    img_files.extend(class_dir.glob(ext))

                for img_file in img_files:
                    try:
                        # å°è¯•åŠ è½½å›¾ç‰‡ä»¥éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
                        with Image.open(img_file) as img:
                            img.verify()  # éªŒè¯å›¾ç‰‡å®Œæ•´æ€§

                        all_data.append(str(img_file))
                        all_labels.append(class_idx)
                        all_domains.append(domain_idx)

                    except Exception as e:
                        print(f"âš ï¸  Skipping corrupted image: {img_file}")
                        continue

        print(
                f"âœ… Dataset loaded: {len(all_data)} samples, {len(set(all_labels))} classes, {len(set(all_domains))} domains")

        # ä¿å­˜åˆ°ç¼“å­˜
        if all_data:  # åªæœ‰åœ¨æœ‰æ•°æ®æ—¶æ‰ä¿å­˜ç¼“å­˜
            data = np.array(all_data)
            labels = np.array(all_labels)
            domains = np.array(all_domains)
            self._save_cache(data, labels, domains)
            return data, labels, domains
        else:
            print("âŒ No data found!")
            return np.array([]), np.array([]), np.array([])

    def get_class_mapping(self):
        """è·å–ç±»åˆ«æ˜ å°„ä¿¡æ¯"""
        class_mapping = {}
        for domain_idx, domain_name in enumerate(self.domain_names):
            domain_path = self.root_dir / domain_name
            if domain_path.exists():
                class_dirs = sorted([d for d in domain_path.iterdir() if d.is_dir()])
                class_mapping[domain_name] = [d.name for d in class_dirs]
        return class_mapping


def generate_dataset(cfg):
    dir_path = Path(f'{cfg['dir_path']}-{cfg["client_num"]}-L{cfg["label_partition"]}-F{cfg["feature_partition"]}')
    dir_path.mkdir(parents=True, exist_ok=True)

    if check(cfg):
        return

    # åˆå§‹åŒ–DomainNetæ•°æ®é›†åŠ è½½å™¨
    dataset = DomainNetLoader()

    # åŠ è½½åŸå§‹æ•°æ®
    X, y, domains = dataset.load_dataset()

    if len(X) == 0:
        raise RuntimeError("No data loaded! Please check if DomainNet dataset exists at the specified path.")

    print(f"Dataset summary:")
    print(f"  Total samples: \t{len(X)}")
    print(f"  Number of classes: \t{len(np.unique(y))}")
    print(f"  Number of domains: \t{len(np.unique(domains))}")
    print(f"  Domain distribution: \t{np.bincount(domains)}")

    # æ›´æ–°é…ç½®
    cfg['class_num'] = len(np.unique(y))
    cfg['domain_num'] = len(np.unique(domains))

    # ä½¿ç”¨åŒå±‚åˆ†å¸ƒæ§åˆ¶è¿›è¡Œæ•°æ®åˆ‡åˆ†
    print("\nApplying dual distribution split...")
    X_clients, y_clients, statistic = split_dual_distribution(X, y, domains, cfg)

    # åˆ‡åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    print("\nSplitting train/test data...")
    train_data, test_data = split_data(X_clients, y_clients, cfg)

    # ä¿å­˜æ–‡ä»¶
    save_file(train_data, test_data, cfg)

    # ä¿å­˜è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶
    statistics_path = dir_path / "client_distribution_statistics.yaml"
    save_detailed_statistics(statistic, str(statistics_path), DOMAIN_NAMES)

    # ä¿å­˜é¢å¤–çš„DomainNetç‰¹å®šä¿¡æ¯
    _save_domainnet_metadata(dir_path, cfg, dataset)


def _save_domainnet_metadata(dir_path, cfg, dataset):
    """ä¿å­˜DomainNetç‰¹å®šçš„å…ƒæ•°æ®"""
    metadata = {
        'domain_names'            : DOMAIN_NAMES,
        'num_domains'             : len(DOMAIN_NAMES),
        'num_classes_per_domain'  : NUM_CLASSES_PER_DOMAIN,
        'class_mapping'           : dataset.get_class_mapping(),
        'dual_distribution_config': {
            'label_partition'  : cfg.get('label_partition', 'uni'),
            'label_alpha'      : cfg.get('label_alpha', 10000),
            'feature_partition': cfg.get('feature_partition', 'uni'),
            'feature_alpha'    : cfg.get('feature_alpha', 10000),
        }
    }

    metadata_path = dir_path / "domainnet_metadata.yaml"
    with metadata_path.open('w') as f:
        yaml.dump(metadata, f, default_flow_style=False)

    print(f"DomainNet metadata saved to: {metadata_path}")


if __name__ == "__main__":
    with Path('config.yaml').open('r') as f:
        config = yaml.load(f.read(), Loader=yaml.Loader)

    # éªŒè¯é…ç½®
    assert config['dir_path'].lower() == 'domainnet', \
        'Dataset name does not match saving dir_path (dataset/config.yaml) !'

    assert config['partition'] == 'dual', \
        'DomainNet requires partition: dual for dual distribution!'

    # ç”Ÿæˆæ•°æ®é›†
    generate_dataset(config)
